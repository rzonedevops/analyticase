# AnalytiCase Simulation Logging System

## Overview

The AnalytiCase simulation suite now includes a comprehensive logging system that organizes all simulation outputs into timestamped folders with a clear directory structure. This ensures that every simulation run is fully traceable, reproducible, and well-documented.

## Directory Structure

Each simulation run creates a timestamped directory with the following structure:

```
results/
└── YYYYMMDD_HHMMSS_[run_name]/
    ├── logs/                    # Execution logs
    │   ├── simulation_main.log  # Main orchestration log
    │   ├── agent_based.log      # Agent-based model log
    │   ├── discrete_event.log   # Discrete-event model log
    │   ├── system_dynamics.log  # System dynamics model log
    │   ├── hyper_gnn.log        # HyperGNN model log
    │   └── case_llm.log         # Case-LLM model log
    ├── data/                    # Raw data and JSON results
    │   ├── complete_results.json
    │   ├── agent_based_results.json
    │   ├── discrete_event_results.json
    │   ├── system_dynamics_results.json
    │   ├── hypergnn_results.json
    │   └── case_llm_results.json
    ├── reports/                 # Human-readable reports
    │   ├── summary_report.txt
    │   └── legal_brief.txt
    ├── visualizations/          # Charts and graphs (future)
    └── manifest.json            # Run metadata and configuration
```

## Usage

### Basic Usage

Run the enhanced simulation suite with default settings:

```bash
python3 simulations/simulation_runner_v2.py
```

This creates a timestamped directory like `results/20251014_050740/`

### Named Runs

Give your simulation run a descriptive name:

```bash
python3 simulations/simulation_runner_v2.py --name "production_run"
```

This creates a directory like `results/20251014_050740_production_run/`

### Custom Output Directory

Specify a custom output directory:

```bash
python3 simulations/simulation_runner_v2.py --output /path/to/custom/output
```

### With Configuration File

Use a JSON configuration file to customize simulation parameters:

```bash
python3 simulations/simulation_runner_v2.py --config config.json --name "custom_config"
```

Example `config.json`:

```json
{
  "agent_based": {
    "num_investigators": 10,
    "num_attorneys": 15,
    "num_judges": 5,
    "num_steps": 200
  },
  "discrete_event": {
    "num_cases": 100,
    "simulation_duration": 730.0
  },
  "system_dynamics": {
    "duration": 730.0,
    "dt": 0.5
  },
  "hyper_gnn": {
    "input_dim": 128,
    "hidden_dim": 64,
    "num_layers": 3
  },
  "case_llm": {
    "model_name": "gpt-4.1-mini",
    "generate_brief": true
  }
}
```

## Log Files

### Main Simulation Log

**File:** `logs/simulation_main.log`

Contains orchestration-level information:
- Simulation suite initialization
- Overall progress
- Summary statistics
- Finalization status

### Model-Specific Logs

Each model has its own dedicated log file:

- `logs/agent_based.log` - Agent interactions, workload distribution, efficiency metrics
- `logs/discrete_event.log` - Event processing, case transitions, timeline
- `logs/system_dynamics.log` - Stock levels, flow rates, policy changes
- `logs/hyper_gnn.log` - Hypergraph construction, embedding updates, community detection
- `logs/case_llm.log` - LLM API calls, entity extraction, outcome prediction

### Log Format

All logs use a consistent format:

```
YYYY-MM-DD HH:MM:SS - logger_name - LEVEL - message
```

Example:
```
2025-10-14 05:07:40 - simulation_agent_based - INFO - Starting agent_based simulation
2025-10-14 05:07:40 - simulation_agent_based - INFO - Configuration: {'num_investigators': 5, 'num_attorneys': 8, 'num_judges': 3, 'num_steps': 100}
```

## Data Files

### Complete Results

**File:** `data/complete_results.json`

Contains all simulation results in a single JSON file with the following structure:

```json
{
  "timestamp": "20251014_050740",
  "results": {
    "agent_based": { ... },
    "discrete_event": { ... },
    "system_dynamics": { ... },
    "hyper_gnn": { ... },
    "case_llm": { ... }
  }
}
```

### Model-Specific Results

Each model's results are also saved separately:

- `data/agent_based_results.json`
- `data/discrete_event_results.json`
- `data/system_dynamics_results.json`
- `data/hypergnn_results.json`
- `data/case_llm_results.json`

## Reports

### Summary Report

**File:** `reports/summary_report.txt`

A human-readable summary of all simulation results, including:
- Timestamp and run directory
- Success/failure counts
- Key insights from each model

### Legal Brief

**File:** `reports/legal_brief.txt`

Generated by the Case-LLM model, contains a detailed legal analysis of the case.

## Manifest File

**File:** `manifest.json`

Contains metadata about the simulation run:

```json
{
  "timestamp": "20251014_050740",
  "run_directory": "/path/to/run/directory",
  "metadata": {
    "purpose": "Comprehensive legal case analysis",
    "models": ["agent_based", "discrete_event", "system_dynamics", "hyper_gnn", "case_llm"],
    "configuration": { ... }
  },
  "directories": {
    "logs": "/path/to/logs",
    "data": "/path/to/data",
    "reports": "/path/to/reports",
    "visualizations": "/path/to/visualizations"
  }
}
```

## Programmatic Usage

You can also use the logging system programmatically in your own scripts:

```python
from simulations.logging_config import setup_logging

# Initialize logger
logger_manager = setup_logging(run_name="my_custom_run")

# Get model-specific logger
model_logger = logger_manager.get_model_logger("my_model")

# Log simulation start
logger_manager.log_simulation_start("my_model", {"param1": "value1"})

# Log messages
model_logger.info("Processing data...")
model_logger.warning("Potential issue detected")
model_logger.error("Error occurred", exc_info=True)

# Save data
data_path = logger_manager.get_data_path("my_results.json")
with open(data_path, 'w') as f:
    json.dump(results, f)

# Log simulation end
logger_manager.log_simulation_end("my_model", results)

# Create manifest
logger_manager.create_run_manifest({"purpose": "Custom analysis"})

# Finalize
logger_manager.finalize()
```

## Benefits

### Traceability
Every simulation run is uniquely identified by its timestamp, making it easy to track and compare different runs.

### Organization
Clear separation of logs, data, reports, and visualizations keeps outputs organized and easy to navigate.

### Reproducibility
The manifest file captures all configuration parameters, enabling exact reproduction of simulation runs.

### Debugging
Detailed logs for each model make it easy to diagnose issues and understand simulation behavior.

### Analysis
Structured JSON output enables programmatic analysis of results across multiple simulation runs.

## Best Practices

1. **Use Descriptive Names**: When running important simulations, use the `--name` flag with a descriptive name
2. **Keep Configuration Files**: Save your configuration files alongside the results for future reference
3. **Archive Old Runs**: Periodically archive old simulation runs to keep the results directory manageable
4. **Check Logs First**: When investigating issues, always check the relevant log files first
5. **Compare Runs**: Use the timestamped directories to compare results across different configurations

## Troubleshooting

### Issue: Permission Denied

**Solution:** Ensure you have write permissions to the output directory.

### Issue: Disk Space

**Solution:** Simulation runs can generate significant data. Monitor disk space and archive old runs.

### Issue: Missing Logs

**Solution:** Check that the logging configuration is properly initialized before running simulations.

## Future Enhancements

- Automatic visualization generation (charts, graphs)
- Database integration for long-term result storage
- Web-based dashboard for viewing simulation results
- Automated comparison reports across multiple runs
- Real-time log streaming for long-running simulations

## Support

For issues or questions about the logging system, please refer to the main repository documentation or open an issue on GitHub.

